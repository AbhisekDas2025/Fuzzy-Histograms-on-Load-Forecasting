{"cells":[{"cell_type":"markdown","metadata":{"id":"NAfyB4oN-E1I"},"source":["# LSTM Model\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1bNOmWzmnrnkHstAoCYS7PzESujI_6WW3"},"executionInfo":{"elapsed":45214,"status":"ok","timestamp":1745431878343,"user":{"displayName":"Abhisek Das","userId":"09120859727457019726"},"user_tz":300},"id":"WeEMZRWknZct","outputId":"56c0e2e0-2fff-4248-ac5f-de35f1fe88c9"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Install necessary libraries\n","!pip install tensorflow pandas numpy scikit-learn matplotlib\n","\n","# Import libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split # Not used directly for final split, but good practice\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from math import sqrt\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","from tensorflow.keras import backend as K # For clearing session\n","\n","# Mount Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# --- Configuration ---\n","DATA_PATH = '/content/drive/My Drive/Non-Thesis Project/Tetouan City Power Consumption.csv'\n","TARGET_MONTHS = [4, 7, 10, 12]  # Months for training and testing\n","TRAIN_DAYS = range(1, 8)     # Days 1-7 of the target months for training\n","# *** MODIFIED: Test days range changed to 8-14 ***\n","TEST_DAYS = range(8, 15)    # Days 8-14 of the target months for testing\n","# ---------------------------------------------------\n","TOLERANCE_PERCENTAGE = 15.0\n","\n","# --- Define Fixed LSTM Hyperparameters (for the 3-layer model) ---\n","FIXED_LSTM_UNITS = 192\n","FIXED_DENSE_UNITS = 128\n","FIXED_DROPOUT = 0.25\n","FIXED_LEARNING_RATE = 0.001\n","# ----------------------------------------------------------------\n","\n","# --- Data Loading and Preprocessing ---\n","print(\"Loading and preprocessing data...\")\n","# Added error handling for file loading\n","try:\n","    data = pd.read_csv(DATA_PATH)\n","except FileNotFoundError:\n","    print(f\"ERROR: Data file not found at {DATA_PATH}\")\n","    raise\n","data.columns = [col.strip().replace(' ', '_').lower() for col in data.columns]\n","zone_columns = [col for col in data.columns if 'power_consumption' in col]\n","print(\"Detected zone columns:\", zone_columns)\n","data['datetime'] = pd.to_datetime(data['datetime'])\n","\n","# Resample the data to hourly intervals FIRST\n","numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n","data = data.set_index('datetime')\n","hourly_data = data[numeric_cols].resample('H').mean()\n","hourly_data.reset_index(inplace=True) # Keep datetime as column temporarily\n","\n","# Create/Recreate temporal features AFTER resampling\n","hourly_data['hour'] = hourly_data['datetime'].dt.hour\n","hourly_data['dayofweek'] = hourly_data['datetime'].dt.dayofweek\n","hourly_data['month'] = hourly_data['datetime'].dt.month\n","hourly_data['dayofmonth'] = hourly_data['datetime'].dt.day # Renamed from 'day' for clarity\n","hourly_data['dayofyear'] = hourly_data['datetime'].dt.dayofyear\n","day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n","hourly_data['day_name'] = hourly_data['dayofweek'].apply(lambda x: day_names[x])\n","month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n","hourly_data['month_name'] = hourly_data['month'].apply(lambda x: month_names[x-1])\n","\n","# Handle potential NaNs from resampling (only for non-target numeric cols)\n","numeric_features_to_fill = [col for col in hourly_data.select_dtypes(include=np.number).columns if col not in zone_columns]\n","for col in numeric_features_to_fill:\n","    if hourly_data[col].isnull().any():\n","        print(f\"Filling NaNs in '{col}' using ffill/bfill/mean...\")\n","        hourly_data[col].fillna(method='ffill', inplace=True)\n","        hourly_data[col].fillna(method='bfill', inplace=True)\n","        if hourly_data[col].isnull().any():\n","            mean_val = hourly_data[col].mean()\n","            print(f\"Warning: Filling remaining NaNs in '{col}' with overall mean ({mean_val:.2f}).\")\n","            hourly_data[col].fillna(mean_val, inplace=True)\n","\n","# Add lag features\n","print(\"Adding lag features...\")\n","lags_to_add = list(range(1, 7)) # Use a list for clarity\n","for lag in lags_to_add:\n","    for zone_col in zone_columns:\n","        if zone_col in hourly_data.columns: hourly_data[f'{zone_col}_lag{lag}'] = hourly_data[zone_col].shift(lag)\n","\n","# Drop rows with NaN values (mainly due to lagging)\n","print(f\"Dropping initial rows with NaNs from lags...\")\n","cols_with_initial_nans = [f'{zc}_lag{lag}' for zc in zone_columns for lag in lags_to_add if f'{zc}_lag{lag}' in hourly_data.columns]\n","original_len = len(hourly_data)\n","hourly_data.dropna(subset=cols_with_initial_nans, inplace=True)\n","print(f\"Dropped {original_len - len(hourly_data)} rows.\")\n","if len(hourly_data) == 0: raise ValueError(\"Not enough data remaining after dropping initial NaN rows from lags.\")\n","\n","print(f\"Data shape after preprocessing: {hourly_data.shape}\")\n","\n","# --- Filtering and Feature Preparation ---\n","print(f\"Filtering and preparing features for TRAIN_DAYS={list(TRAIN_DAYS)} and TEST_DAYS={list(TEST_DAYS)}...\")\n","# Set index AFTER preprocessing and BEFORE filtering\n","hourly_data.set_index('datetime', inplace=True)\n","\n","# Filter using the defined day ranges\n","# *** CORRECTED: Use .day instead of .dayofmonth for DatetimeIndex ***\n","train_data = hourly_data[(hourly_data.index.month.isin(TARGET_MONTHS)) & (hourly_data.index.day.isin(TRAIN_DAYS))].copy()\n","test_data = hourly_data[(hourly_data.index.month.isin(TARGET_MONTHS)) & (hourly_data.index.day.isin(TEST_DAYS))].copy()\n","# -------------------------------------------------------------------\n","\n","\n","if train_data.empty: raise ValueError(\"Train data is empty after filtering. Check TARGET_MONTHS/TRAIN_DAYS.\")\n","if test_data.empty: raise ValueError(f\"Test data is empty after filtering. Check TARGET_MONTHS/TEST_DAYS (currently {list(TEST_DAYS)}).\")\n","\n","print(f\"Train shape: {train_data.shape}, Test shape: {test_data.shape}\")\n","\n","# Prepare features (keep only numeric features for this basic model)\n","train_features = train_data[[col for col in train_data.columns if col not in zone_columns]]\n","test_features = test_data[[col for col in test_data.columns if col not in zone_columns]]\n","# Drop the string name columns if they exist\n","train_features = train_features.drop(columns=['day_name', 'month_name'], errors='ignore')\n","test_features = test_features.drop(columns=['day_name', 'month_name'], errors='ignore')\n","\n","# Select only numeric features now for scaling\n","train_features_numeric = train_features.select_dtypes(include=np.number)\n","test_features_numeric = test_features.select_dtypes(include=np.number)\n","\n","# Extract targets\n","train_targets = train_data[zone_columns]\n","test_targets = test_data[zone_columns]\n","\n","# Align columns based on numeric features selected\n","train_cols = train_features_numeric.columns.tolist()\n","test_cols = test_features_numeric.columns.tolist()\n","\n","print(f\"Initial number of numeric features in train: {len(train_cols)}\")\n","print(f\"Initial number of numeric features in test: {len(test_cols)}\")\n","\n","# Align columns - Crucial step\n","missing_in_test = set(train_cols) - set(test_cols)\n","for c in missing_in_test:\n","    print(f\"Adding missing column '{c}' to test set (filled with 0).\")\n","    test_features_numeric[c] = 0\n","\n","extra_in_test = set(test_cols) - set(train_cols)\n","if extra_in_test:\n","    print(f\"Dropping extra columns from test set: {extra_in_test}\")\n","    test_features_numeric = test_features_numeric.drop(columns=list(extra_in_test))\n","\n","# Final alignment\n","try:\n","    test_features_numeric = test_features_numeric[train_cols] # Reorder test columns to match train\n","    print(f\"Final number of features in train set: {len(train_features_numeric.columns)}\")\n","    print(f\"Final number of features in test set (aligned): {len(test_features_numeric.columns)}\")\n","    # print(\"Features:\", train_cols) # Uncomment to see list of features\n","except KeyError as e:\n","    print(f\"ERROR: Column alignment failed. Missing columns in test set after adding: {e}\")\n","    print(\"Train columns:\", train_cols)\n","    print(\"Test columns before alignment:\", test_cols)\n","    raise\n","\n","# Check for NaN/inf values after preparation\n","if train_features_numeric.isnull().values.any() or np.isinf(train_features_numeric.values).any():\n","    print(\"ERROR: NaN or Inf found in training features AFTER preparation. Check preprocessing.\")\n","    print(\"Columns with NaNs/Infs in Training Features:\\n\", train_features_numeric.isin([np.nan, np.inf, -np.inf]).sum())\n","    raise ValueError(\"NaN/Inf in training features.\")\n","if test_features_numeric.isnull().values.any() or np.isinf(test_features_numeric.values).any():\n","    print(\"ERROR: NaN or Inf found in test features AFTER preparation. Check preprocessing.\")\n","    print(\"Columns with NaNs/Infs in Test Features:\\n\", test_features_numeric.isin([np.nan, np.inf, -np.inf]).sum())\n","    raise ValueError(\"NaN/Inf in test features.\")\n","\n","\n","# --- Scaling and Reshaping ---\n","print(\"Scaling and reshaping data...\")\n","scaler = MinMaxScaler() # Feature scaler\n","target_scaler = StandardScaler() # Target scaler\n","\n","train_features_scaled = scaler.fit_transform(train_features_numeric)\n","test_features_scaled = scaler.transform(test_features_numeric) # Use transform only on test\n","\n","train_targets_scaled = target_scaler.fit_transform(train_targets)\n","test_targets_scaled = target_scaler.transform(test_targets) # Use transform only on test\n","\n","# Reshape for LSTM [samples, timesteps, features] - assuming 1 timestep\n","train_features_reshaped = train_features_scaled.reshape((train_features_scaled.shape[0], 1, train_features_scaled.shape[1]))\n","test_features_reshaped = test_features_scaled.reshape((test_features_scaled.shape[0], 1, test_features_scaled.shape[1]))\n","\n","print(f\"Train Features Reshaped: {train_features_reshaped.shape}\")\n","print(f\"Test Features Reshaped: {test_features_reshaped.shape}\")\n","print(f\"Train Targets Scaled Shape: {train_targets_scaled.shape}\")\n","print(f\"Test Targets Scaled Shape: {test_targets_scaled.shape}\")\n","\n","\n","# --- Model Definition (Modified for 3 Layers) ---\n","def build_lstm_model_3layer(lstm_units, dense_units, dropout_rate, lr, n_features, n_outputs):\n","    \"\"\"\n","    Builds a 3-layer Bidirectional LSTM model for the basic feature set.\n","    \"\"\"\n","    model = Sequential([\n","        # Layer 1\n","        Bidirectional(LSTM(lstm_units, return_sequences=True, input_shape=(1, n_features))),\n","        Dropout(dropout_rate),\n","        BatchNormalization(),\n","        # Layer 2\n","        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n","        Dropout(dropout_rate),\n","        BatchNormalization(),\n","        # Layer 3\n","        Bidirectional(LSTM(lstm_units, return_sequences=False)),\n","        Dropout(dropout_rate),\n","        BatchNormalization(),\n","        # Dense Layers\n","        Dense(dense_units, activation='relu'),\n","        Dense(n_outputs) # Linear activation for regression\n","    ])\n","    optimizer = Adam(learning_rate=lr)\n","    model.compile(loss='mean_squared_error', optimizer=optimizer)\n","    return model\n","\n","# --- Model Building and Training (Using Fixed Parameters - 3 Layers) ---\n","print(\"\\nBuilding and training 3-Layer LSTM model (Basic Features)...\")\n","n_features = train_features_reshaped.shape[2]\n","n_outputs = train_targets_scaled.shape[1]\n","K.clear_session() # Clear previous models from memory\n","\n","model = build_lstm_model_3layer(\n","    FIXED_LSTM_UNITS,\n","    FIXED_DENSE_UNITS,\n","    FIXED_DROPOUT,\n","    FIXED_LEARNING_RATE,\n","    n_features,\n","    n_outputs\n",")\n","\n","print(\"Using Fixed Hyperparameters:\")\n","print(f\"  LSTM Units (x3 Layers): {FIXED_LSTM_UNITS}\")\n","print(f\"  Dense Units: {FIXED_DENSE_UNITS}\")\n","print(f\"  Dropout Rate: {FIXED_DROPOUT:.3f}\")\n","print(f\"  Learning Rate: {FIXED_LEARNING_RATE:.6f}\")\n","\n","model.summary()\n","\n","# --- Training Setup (Callbacks and Fit) ---\n","early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-6, verbose=1)\n","\n","history = model.fit(\n","    train_features_reshaped,\n","    train_targets_scaled,\n","    epochs=150,\n","    batch_size=64,\n","    validation_split=0.2,\n","    callbacks=[early_stopping, reduce_lr],\n","    verbose=1\n",")\n","\n","# Plot Training Loss\n","plt.figure(figsize=(10, 6))\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title(f'Model Loss (Basic Features, 3x{FIXED_LSTM_UNITS} LSTM)') # Updated title\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss (MSE)')\n","plt.legend()\n","plt.grid(True)\n","# Adjust ylim for better visualization\n","if len(history.history['loss']) > 5:\n","    max_loss_val = max(history.history['loss'][5:] + history.history['val_loss'][5:])\n","    min_loss_val = min(history.history['loss'] + history.history['val_loss'])\n","    plt.ylim(bottom=max(0, min_loss_val * 0.9), top=max_loss_val * 1.1)\n","elif len(history.history['loss']) > 0:\n","    max_loss_val = max(history.history['loss'] + history.history['val_loss'])\n","    min_loss_val = min(history.history['loss'] + history.history['val_loss'])\n","    plt.ylim(bottom=max(0, min_loss_val * 0.9), top=max_loss_val * 1.1)\n","else:\n","    plt.ylim(bottom=0)\n","plt.show()\n","\n","if hasattr(early_stopping, 'stopped_epoch') and early_stopping.stopped_epoch > 0 and early_stopping.restore_best_weights:\n","    print(f\"Training stopped early at epoch {early_stopping.stopped_epoch + 1}. Best weights restored.\")\n","elif hasattr(early_stopping, 'stopped_epoch') and early_stopping.stopped_epoch > 0:\n","     print(f\"Training stopped early at epoch {early_stopping.stopped_epoch + 1}, but best weights might not have been restored.\")\n","else:\n","    print(\"Training completed all epochs or early stopping did not trigger.\")\n","\n","\n","# --- Evaluation ---\n","print(\"\\nEvaluating model on the test set...\")\n","test_predictions_scaled = model.predict(test_features_reshaped)\n","\n","# Inverse transform predictions and actuals\n","actual_test_targets = target_scaler.inverse_transform(test_targets_scaled)\n","predicted_test_targets = target_scaler.inverse_transform(test_predictions_scaled)\n","\n","# Clip negative predictions\n","predicted_test_targets[predicted_test_targets < 0] = 0\n","\n","# --- Create plotting dataframe ---\n","plot_ready = False\n","# Check length using the index of test_data and the length of predictions\n","if len(test_data.index) == len(actual_test_targets):\n","    # Create DataFrames for actual and predicted values using the test data index\n","    actual_df = pd.DataFrame(actual_test_targets, index=test_data.index, columns=zone_columns)\n","    pred_df = pd.DataFrame(predicted_test_targets, index=test_data.index, columns=[f\"{zone}_predicted\" for zone in zone_columns])\n","\n","    # Combine index, actuals, and predictions into a new DataFrame for plotting\n","    plot_test_df = pd.concat([actual_df, pred_df], axis=1)\n","    # Re-add time components needed for plotting titles/labels directly from the index\n","    plot_test_df['hour'] = plot_test_df.index.hour\n","    plot_test_df['day'] = plot_test_df.index.day # Use .day\n","    plot_test_df['month_name'] = plot_test_df.index.strftime('%B')\n","    plot_test_df['day_name'] = plot_test_df.index.day_name()\n","\n","    plot_ready = True\n","    print(f\"Plotting dataframe created successfully.\")\n","else:\n","    print(f\"Error: Length mismatch! Original Test DF index length {len(test_data.index)} vs Predictions length {len(actual_test_targets)}. Skipping plotting.\")\n","    plot_test_df = None # Ensure it's None if not created\n","\n","\n","# --- Plotting Results ---\n","if plot_ready and plot_test_df is not None:\n","    print(f\"Plotting daily forecasts (Basic Features, 3x{FIXED_LSTM_UNITS} LSTM) for days {list(TEST_DAYS)}...\") # Updated description\n","    # Get unique month names present in the test data index\n","    unique_month_names = plot_test_df['month_name'].unique()\n","\n","    for zone in zone_columns:\n","        zone_name_plot = zone.replace(\"_power_consumption\", \"\").replace(\"zone_\", \"Zone \").capitalize()\n","\n","        for month_name in unique_month_names:\n","            month_data = plot_test_df[plot_test_df['month_name'] == month_name]\n","            if month_data.empty: continue\n","\n","            available_days_in_plot_df = month_data['day'].unique()\n","            # Ensure plot_days only includes days defined in TEST_DAYS that are actually present in the filtered test data\n","            plot_days = sorted([d for d in TEST_DAYS if d in available_days_in_plot_df])\n","\n","            for day in plot_days:\n","                # Filter data specifically for this day within the month\n","                filtered_data = month_data[month_data['day'] == day]\n","                if filtered_data.empty: continue # Should not happen if day is in plot_days, but safe check\n","\n","                actual_col_name = zone\n","                predicted_col_name = f\"{zone}_predicted\"\n","                if actual_col_name not in filtered_data.columns or predicted_col_name not in filtered_data.columns:\n","                    print(f\"Warning: Column missing for {zone}, {month_name}, {day}. Skipping plot.\")\n","                    continue\n","\n","                actual_day = filtered_data[actual_col_name]\n","                predicted_day = filtered_data[predicted_col_name]\n","\n","                # Ensure 1D input for metrics\n","                if not isinstance(actual_day, pd.Series) or not isinstance(predicted_day, pd.Series): continue\n","                if actual_day.ndim > 1 or predicted_day.ndim > 1: continue\n","\n","                # Calculate metrics safely\n","                try:\n","                    daily_rmse = sqrt(mean_squared_error(actual_day, predicted_day))\n","                    daily_mae = mean_absolute_error(actual_day, predicted_day)\n","                    daily_r2 = r2_score(actual_day, predicted_day)\n","                except ValueError as e_met:\n","                    print(f\"Warning: Metric calculation error for {zone}, {month_name}, {day}: {e_met}\")\n","                    daily_rmse, daily_mae, daily_r2 = np.nan, np.nan, np.nan\n","                except Exception as e_gen:\n","                    print(f\"Unexpected error calculating metrics for {zone}, {month_name}, {day}: {e_gen}\")\n","                    daily_rmse, daily_mae, daily_r2 = np.nan, np.nan, np.nan\n","\n","                plt.figure(figsize=(14, 7))\n","                bar_width = 0.35\n","                x = np.arange(len(filtered_data)) # Should be 24 hours\n","                plt.bar(x - bar_width/2, actual_day, width=bar_width, label='Actual', color='black', alpha=0.8)\n","                plt.bar(x + bar_width/2, predicted_day, width=bar_width, label='Predicted', color='orange', alpha=0.8)\n","\n","                day_name_str = filtered_data['day_name'].iloc[0] if not filtered_data.empty else \"UnknownDay\"\n","                plt.title(f\"{zone_name_plot} Load Forecast (Basic, 3x{FIXED_LSTM_UNITS} LSTM) - {day_name_str}, {month_name} {day}\", fontsize=14) # Updated title\n","                plt.xlabel('Hour of Day', fontsize=12)\n","                plt.ylabel('Power Consumption', fontsize=12)\n","                plt.xticks(x, filtered_data['hour'], rotation=45, ha='right')\n","                plt.legend(fontsize=10)\n","                plt.grid(axis='y', linestyle='--', alpha=0.7)\n","                plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout\n","\n","                stats_text = (f\"Daily Metrics:\\n\"\n","                              f\"RMSE: {daily_rmse:.2f}\\n\"\n","                              f\"MAE:  {daily_mae:.2f}\\n\"\n","                              f\"R²:   {daily_r2:.3f}\")\n","                plt.text(1.02, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,\n","                         verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', facecolor='orange', alpha=0.6))\n","                plt.show()\n","else:\n","    print(\"Skipping plots due to data length mismatch or plotting dataframe creation failed.\")\n","\n","# --- Final Evaluation Metrics ---\n","print(f\"\\n--- Test Set Evaluation Metrics (Basic Features, 3x{FIXED_LSTM_UNITS} LSTM - Per Zone) ---\") # Updated description\n","all_actuals_flat = []\n","all_predictions_flat = []\n","\n","for i, zone in enumerate(zone_columns):\n","    zone_name_print = zone.replace(\"zone_\", \"Zone_\").replace(\"_power_consumption\", \"_\")\n","    actual = actual_test_targets[:, i]\n","    predicted = predicted_test_targets[:, i]\n","    all_actuals_flat.extend(actual)\n","    all_predictions_flat.extend(predicted)\n","\n","    rmse = sqrt(mean_squared_error(actual, predicted))\n","    mae = mean_absolute_error(actual, predicted)\n","    try: r2 = r2_score(actual, predicted)\n","    except ValueError: r2 = np.nan\n","\n","    print(f\"\\n--- {zone_name_print} ---\")\n","    print(f\"  RMSE: {rmse:.2f}\")\n","    print(f\"  MAE: {mae:.2f}\")\n","    print(f\"  R² Score: {r2:.4f}\")\n","\n","print(f\"\\n--- Overall Model Evaluation (Basic Features, 3x{FIXED_LSTM_UNITS} LSTM - All Zones Combined) ---\") # Updated description\n","all_actuals_flat = np.array(all_actuals_flat)\n","all_predictions_flat = np.array(all_predictions_flat)\n","\n","overall_rmse = sqrt(mean_squared_error(all_actuals_flat, all_predictions_flat))\n","overall_mae = mean_absolute_error(all_actuals_flat, all_predictions_flat)\n","try: overall_r2 = r2_score(all_actuals_flat, all_predictions_flat)\n","except ValueError: overall_r2 = np.nan\n","\n","# Calculate Tolerance Accuracy Safely\n","abs_error = np.abs(all_actuals_flat - all_predictions_flat)\n","non_zero_actuals = all_actuals_flat[all_actuals_flat > 1e-6]\n","mean_nonzero_actual = np.mean(non_zero_actuals) if len(non_zero_actuals) > 0 else 1.0\n","\n","absolute_min_tolerance = 10 # Use the same minimum absolute tolerance\n","percentage_tolerance_value = (TOLERANCE_PERCENTAGE / 100.0) * np.abs(all_actuals_flat)\n","tolerance_threshold = np.maximum(percentage_tolerance_value, absolute_min_tolerance)\n","\n","is_within_tolerance = abs_error <= tolerance_threshold\n","tolerance_accuracy = np.mean(is_within_tolerance) * 100\n","\n","print(f\"  Overall RMSE: {overall_rmse:.2f}\")\n","print(f\"  Overall MAE: {overall_mae:.2f}\")\n","print(f\"  Overall R² Score: {overall_r2:.4f}\")\n","print(f\"\\n  Tolerance-Based Accuracy ({TOLERANCE_PERCENTAGE}%): {tolerance_accuracy:.2f}%\")\n","print(f\"     (Accuracy defined as % of predictions within +/- {TOLERANCE_PERCENTAGE}% of actual, with min absolute tolerance of {absolute_min_tolerance} units)\")\n","print(\"----------------------------------------------------------\")\n","\n","print(\"\\nProcessing complete.\")"]},{"cell_type":"markdown","source":["#Hybrid Fuzzy-LSTM Model"],"metadata":{"id":"QWZCXOebWt9n"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install tensorflow scikit-fuzzy scikit-learn pandas numpy matplotlib\n","\n","# Import libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split # Keep for final validation split\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from math import sqrt, pi # Removed log10, floor as they weren't used after opt removal\n","import matplotlib.pyplot as plt\n","import skfuzzy as fuzz\n","from skfuzzy import control as ctrl\n","# --- FCM Import ---\n","from skfuzzy.cluster import cmeans, cmeans_predict\n","# ------------------\n","from google.colab import drive\n","import random\n","import warnings\n","# TensorFlow/Keras\n","from tensorflow.keras import backend as K # For clearing session\n","\n","# Mount Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# --- Configuration ---\n","DATA_PATH = '/content/drive/My Drive/Non-Thesis Project/Tetouan City Power Consumption.csv'\n","TARGET_MONTHS = [4, 7, 10, 12]\n","TRAIN_DAYS = range(1, 8)      # Days 1-7 for training\n","# *** MODIFIED: Test days range changed to 8-14 ***\n","TEST_DAYS = range(8, 15)     # Days 8-14 for testing\n","# ---------------------------------------------------\n","FUZZY_POWER_MAX = 50000\n","TEMP_MIN = 0\n","TEMP_MAX = 45\n","TOLERANCE_PERCENTAGE = 15.0\n","ROLLING_WINDOWS = [6, 24]\n","\n","# --- FCM Configuration ---\n","FCM_N_CLUSTERS = 7 # Number of fuzzy clusters to identify (Hyperparameter)\n","FCM_M = 2.0        # Fuzziness parameter (Standard default)\n","FCM_ERROR = 0.005  # Stopping criterion error\n","FCM_MAXITER = 1000 # Max iterations for FCM training\n","\n","# --- Define Fixed LSTM Hyperparameters ---\n","FIXED_LSTM_UNITS = 192 # Units for EACH of the 3 LSTM layers\n","FIXED_DENSE_UNITS = 128\n","FIXED_DROPOUT = 0.25\n","FIXED_LEARNING_RATE = 0.001\n","# --------------------------------------\n","\n","# --- Fuzzy Logic Setup (For Feature Engineering - Keep Enhanced Version) ---\n","# 1. Hour\n","hour_universe = np.arange(0, 24, 1); hour_fuzzy = ctrl.Antecedent(hour_universe, 'hour_fuzzy')\n","hour_fuzzy['night'] = fuzz.trapmf(hour_fuzzy.universe, [0, 0, 4, 6]); hour_fuzzy['morning'] = fuzz.trimf(hour_fuzzy.universe, [5, 9, 12]); hour_fuzzy['afternoon'] = fuzz.trimf(hour_fuzzy.universe, [11, 15, 18]); hour_fuzzy['evening'] = fuzz.trapmf(hour_fuzzy.universe, [17, 20, 23, 23])\n","# 2. Lag1 Power\n","fuzzy_power_lag1 = {}\n","def setup_fuzzy_power_lag1(zone_cols, power_max_val):\n","    global fuzzy_power_lag1; power_min = 0; power_max = power_max_val; power_step = max(1, (power_max - power_min) / 100); power_universe = np.arange(power_min, power_max + power_step, power_step)\n","    for zone in zone_cols:\n","        var_name = f\"{zone}_lag1_fuzzy\"; power_lag1_fuzzy = ctrl.Antecedent(power_universe, var_name)\n","        low_thresh = power_max * 0.3; med_thresh_1 = power_max * 0.25; med_thresh_2 = power_max * 0.6; high_thresh = power_max * 0.55\n","        power_lag1_fuzzy['low'] = fuzz.trapmf(power_universe, [power_min, power_min, low_thresh*0.8, low_thresh]); power_lag1_fuzzy['medium'] = fuzz.trimf(power_universe, [med_thresh_1, (med_thresh_1+med_thresh_2)/2 , med_thresh_2]); power_lag1_fuzzy['high'] = fuzz.trapmf(power_universe, [high_thresh, high_thresh*1.1, power_max, power_max])\n","        fuzzy_power_lag1[zone] = power_lag1_fuzzy\n","# 3. Day Type\n","daytype_universe = np.arange(0, 7, 1); day_type_fuzzy = ctrl.Antecedent(daytype_universe, 'day_type_fuzzy')\n","day_type_fuzzy['weekday'] = fuzz.trapmf(daytype_universe, [0, 0, 4, 4.5]); day_type_fuzzy['weekend'] = fuzz.trapmf(daytype_universe, [4.5, 5, 6, 6])\n","# 4. Season\n","month_universe_season = np.arange(1, 13, 1); season_fuzzy = ctrl.Antecedent(month_universe_season, 'season_fuzzy')\n","season_fuzzy['spring'] = fuzz.trimf(month_universe_season, [3, 4, 5])\n","season_fuzzy['summer'] = fuzz.trimf(month_universe_season, [6, 7, 8])\n","season_fuzzy['autumn'] = fuzz.trimf(month_universe_season, [9, 10, 11])\n","season_fuzzy['winter'] = fuzz.trimf(month_universe_season, [11, 12, 13]) # Corrected previously\n","# 5. Temperature\n","temp_universe = np.arange(TEMP_MIN, TEMP_MAX + 1, 1); temp_fuzzy = ctrl.Antecedent(temp_universe, 'temp_fuzzy')\n","temp_fuzzy['cold'] = fuzz.trapmf(temp_universe, [TEMP_MIN, TEMP_MIN, 5, 12]); temp_fuzzy['mild'] = fuzz.trimf(temp_universe, [8, 15, 22]); temp_fuzzy['warm'] = fuzz.trimf(temp_universe, [18, 25, 32]); temp_fuzzy['hot'] = fuzz.trapmf(temp_universe, [28, 35, TEMP_MAX, TEMP_MAX])\n","\n","# --- Enhanced Fuzzification Function (Includes DayType, Season, Temp) ---\n","def fuzzify_data_enhanced(row, zone_cols):\n","    memberships = {}\n","    # Hour\n","    hour_val = row.get('hour_original', np.nan)\n","    if pd.notna(hour_val): memberships['hour_night'] = fuzz.interp_membership(hour_universe, hour_fuzzy['night'].mf, hour_val); memberships['hour_morning'] = fuzz.interp_membership(hour_universe, hour_fuzzy['morning'].mf, hour_val); memberships['hour_afternoon'] = fuzz.interp_membership(hour_universe, hour_fuzzy['afternoon'].mf, hour_val); memberships['hour_evening'] = fuzz.interp_membership(hour_universe, hour_fuzzy['evening'].mf, hour_val)\n","    else: memberships['hour_night'] = 0.0; memberships['hour_morning'] = 0.0; memberships['hour_afternoon'] = 0.0; memberships['hour_evening'] = 0.0\n","    # Lag1 Power\n","    for zone in zone_cols:\n","        lag1_col = f'{zone}_lag1'; power_val_lag1 = row.get(lag1_col, np.nan)\n","        if zone in fuzzy_power_lag1:\n","            fuzz_var = fuzzy_power_lag1[zone]; power_univ = fuzz_var.universe\n","            if pd.notna(power_val_lag1): power_val_lag1_clipped = np.clip(power_val_lag1, power_univ.min(), power_univ.max()); memberships[f'{zone}_lag1_low'] = fuzz.interp_membership(power_univ, fuzz_var['low'].mf, power_val_lag1_clipped); memberships[f'{zone}_lag1_medium'] = fuzz.interp_membership(power_univ, fuzz_var['medium'].mf, power_val_lag1_clipped); memberships[f'{zone}_lag1_high'] = fuzz.interp_membership(power_univ, fuzz_var['high'].mf, power_val_lag1_clipped)\n","            else: memberships[f'{zone}_lag1_low'] = 0.0; memberships[f'{zone}_lag1_medium'] = 0.0; memberships[f'{zone}_lag1_high'] = 0.0\n","        else: memberships[f'{zone}_lag1_low'] = 0.0; memberships[f'{zone}_lag1_medium'] = 0.0; memberships[f'{zone}_lag1_high'] = 0.0\n","    # Day Type\n","    dayofweek_val = row.get('dayofweek', np.nan)\n","    if pd.notna(dayofweek_val): memberships['day_weekday'] = fuzz.interp_membership(daytype_universe, day_type_fuzzy['weekday'].mf, dayofweek_val); memberships['day_weekend'] = fuzz.interp_membership(daytype_universe, day_type_fuzzy['weekend'].mf, dayofweek_val)\n","    else: memberships['day_weekday'] = 0.0; memberships['day_weekend'] = 0.0\n","    # Season\n","    month_val = row.get('month', np.nan)\n","    try:\n","        if pd.notna(month_val):\n","            memberships['season_spring'] = fuzz.interp_membership(month_universe_season, season_fuzzy['spring'].mf, month_val)\n","            memberships['season_summer'] = fuzz.interp_membership(month_universe_season, season_fuzzy['summer'].mf, month_val)\n","            memberships['season_autumn'] = fuzz.interp_membership(month_universe_season, season_fuzzy['autumn'].mf, month_val)\n","            memberships['season_winter'] = fuzz.interp_membership(month_universe_season, season_fuzzy['winter'].mf, month_val)\n","        else:\n","            memberships['season_spring'], memberships['season_summer'], memberships['season_autumn'], memberships['season_winter'] = 0.0, 0.0, 0.0, 0.0\n","    except ValueError:\n","        print(f\"Warning: Could not calculate seasonal membership for month {month_val}\")\n","        memberships['season_spring'], memberships['season_summer'], memberships['season_autumn'], memberships['season_winter'] = 0.0, 0.0, 0.0, 0.0\n","\n","    # Temperature\n","    temp_val = row.get('temperature', np.nan)\n","    if pd.notna(temp_val): temp_val_clipped = np.clip(temp_val, temp_universe.min(), temp_universe.max()); memberships['temp_cold'] = fuzz.interp_membership(temp_universe, temp_fuzzy['cold'].mf, temp_val_clipped); memberships['temp_mild'] = fuzz.interp_membership(temp_universe, temp_fuzzy['mild'].mf, temp_val_clipped); memberships['temp_warm'] = fuzz.interp_membership(temp_universe, temp_fuzzy['warm'].mf, temp_val_clipped); memberships['temp_hot'] = fuzz.interp_membership(temp_universe, temp_fuzzy['hot'].mf, temp_val_clipped)\n","    else: memberships['temp_cold'] = 0.0; memberships['temp_mild'] = 0.0; memberships['temp_warm'] = 0.0; memberships['temp_hot'] = 0.0\n","    return pd.Series(memberships)\n","\n","# --- Data Loading and Preprocessing (with FCM + Enhanced Fuzzy) ---\n","print(\"Loading and preprocessing data...\")\n","# Added error handling for file loading\n","try:\n","    data = pd.read_csv(DATA_PATH)\n","except FileNotFoundError:\n","    print(f\"ERROR: Data file not found at {DATA_PATH}\")\n","    raise\n","data.columns = [col.strip().replace(' ', '_').lower() for col in data.columns]; zone_columns = [col for col in data.columns if 'power_consumption' in col]; data['datetime'] = pd.to_datetime(data['datetime'])\n","setup_fuzzy_power_lag1(zone_columns, FUZZY_POWER_MAX)\n","numeric_cols = data.select_dtypes(include=np.number).columns.tolist(); data = data.set_index('datetime'); hourly_data = data[numeric_cols].resample('H').mean(); hourly_data.reset_index(inplace=True)\n","\n","print(\"Engineering base features (time, weather)...\")\n","hourly_data['hour_original'] = hourly_data['datetime'].dt.hour; hourly_data['dayofweek'] = hourly_data['datetime'].dt.dayofweek; hourly_data['month'] = hourly_data['datetime'].dt.month; hourly_data['dayofyear'] = hourly_data['datetime'].dt.dayofyear; hourly_data['day'] = hourly_data['datetime'].dt.day # Use .day here\n","weather_cols = ['temperature', 'humidity', 'wind_speed', 'general_diffuse_flows', 'diffuse_flows']\n","for col in weather_cols:\n","    if col in hourly_data.columns:\n","        hourly_data[col].fillna(method='ffill', inplace=True)\n","        hourly_data[col].fillna(method='bfill', inplace=True)\n","        if hourly_data[col].isnull().any():\n","            mean_val = hourly_data[col].mean()\n","            print(f\"Warning: Filling remaining NaNs in '{col}' with overall mean ({mean_val:.2f}).\")\n","            hourly_data[col].fillna(mean_val, inplace=True)\n","    else:\n","        print(f\"Warning: Weather column '{col}' not found in data.\")\n","\n","# Updated holiday definition for clarity\n","holidays_2017 = {\n","    1: [1],\n","     12: [1] # Add more if known\n","}\n","hourly_data['is_holiday'] = 0\n","for month, days in holidays_2017.items():\n","    hourly_data.loc[(hourly_data['month'] == month) & (hourly_data['day'].isin(days)), 'is_holiday'] = 1\n","print(f\"Identified {hourly_data['is_holiday'].sum()} holiday hours based on defined list.\")\n","\n","print(\"Engineering cyclical time features...\")\n","hourly_data['hour_sin'] = np.sin(2 * pi * hourly_data['hour_original'] / 24.0); hourly_data['hour_cos'] = np.cos(2 * pi * hourly_data['hour_original'] / 24.0); hourly_data['dayofweek_sin'] = np.sin(2 * pi * hourly_data['dayofweek'] / 7.0); hourly_data['dayofweek_cos'] = np.cos(2 * pi * hourly_data['dayofweek'] / 7.0); hourly_data['month_sin'] = np.sin(2 * pi * hourly_data['month'] / 12.0); hourly_data['month_cos'] = np.cos(2 * pi * hourly_data['month'] / 12.0); hourly_data['dayofyear_sin'] = np.sin(2 * pi * hourly_data['dayofyear'] / 365.25); hourly_data['dayofyear_cos'] = np.cos(2 * pi * hourly_data['dayofyear'] / 365.25)\n","\n","print(f\"Adding rolling mean features (windows: {ROLLING_WINDOWS})...\");\n","for zone_col in zone_columns:\n","    if zone_col in hourly_data.columns:\n","      for window in ROLLING_WINDOWS: hourly_data[f'{zone_col}_roll_mean_{window}h'] = hourly_data[zone_col].rolling(window=window, min_periods=1).mean().shift(1)\n","    else:\n","        print(f\"Warning: Zone column '{zone_col}' not found for rolling mean calculation.\")\n","\n","print(\"Adding lag features (including daily and weekly)...\"); lags_to_add = list(range(1, 7)) + [24, 168]\n","for lag in lags_to_add:\n","    for zone_col in zone_columns:\n","        if zone_col in hourly_data.columns: hourly_data[f'{zone_col}_lag{lag}'] = hourly_data[zone_col].shift(lag)\n","        else:\n","             print(f\"Warning: Zone column '{zone_col}' not found for lag {lag} calculation.\")\n","\n","\n","cols_with_initial_nans = []; cols_with_initial_nans.extend([f'{zc}_lag{lag}' for zc in zone_columns for lag in lags_to_add if f'{zc}_lag{lag}' in hourly_data.columns]); cols_with_initial_nans.extend([f'{zc}_roll_mean_{w}h' for zc in zone_columns for w in ROLLING_WINDOWS if f'{zc}_roll_mean_{w}h' in hourly_data.columns])\n","print(f\"Dropping initial rows with NaNs from lags/rolling features...\"); original_len = len(hourly_data); hourly_data.dropna(subset=cols_with_initial_nans, inplace=True); print(f\"Dropped {original_len - len(hourly_data)} rows.\");\n","if len(hourly_data) == 0: raise ValueError(\"Not enough data remaining after dropping initial NaN rows.\")\n","\n","# --- Fuzzy C-Means Clustering ---\n","print(\"\\n--- Applying Fuzzy C-Means Clustering ---\")\n","fcm_base_zone = 'zone_1_power_consumption' if 'zone_1_power_consumption' in zone_columns else zone_columns[0]\n","print(f\"Using '{fcm_base_zone}' lags as representative for FCM.\")\n","\n","fcm_feature_cols = [\n","    f'{fcm_base_zone}_lag1', f'{fcm_base_zone}_lag24', f'{fcm_base_zone}_lag168',\n","    'temperature', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos'\n","]\n","fcm_feature_cols = [col for col in fcm_feature_cols if col in hourly_data.columns]\n","fcm_membership_cols = []\n","if len(fcm_feature_cols) < 3:\n","    print(f\"Warning: Only {len(fcm_feature_cols)} suitable features found for FCM clustering: {fcm_feature_cols}. Skipping FCM.\")\n","else:\n","    print(f\"Features selected for FCM: {fcm_feature_cols}\")\n","    if hourly_data[fcm_feature_cols].isnull().values.any():\n","        print(\"Warning: NaNs detected in FCM feature columns. Attempting to fill with column means.\")\n","        for col in fcm_feature_cols:\n","             if hourly_data[col].isnull().any():\n","                 mean_val = hourly_data[col].mean()\n","                 hourly_data[col].fillna(mean_val, inplace=True)\n","                 print(f\"Filled NaNs in '{col}' with mean {mean_val:.2f}\")\n","        if hourly_data[fcm_feature_cols].isnull().values.any():\n","             print(\"ERROR: NaNs still present in FCM features after filling. Skipping FCM.\")\n","             fcm_data = None\n","        else:\n","             fcm_data = hourly_data[fcm_feature_cols].values.T\n","    else:\n","        fcm_data = hourly_data[fcm_feature_cols].values.T\n","\n","    if 'fcm_data' in locals() and fcm_data is not None:\n","        # Ensure datetime index exists for filtering BEFORE FCM training filter\n","        if 'datetime' in hourly_data.columns:\n","            hourly_data['datetime'] = pd.to_datetime(hourly_data['datetime'])\n","            # *** CORRECTED HERE: Set drop=True (or remove it as it's default) ***\n","            hourly_data.set_index('datetime', inplace=True, drop=True)\n","        else:\n","            # If datetime column doesn't exist, maybe index is already datetime? Check.\n","             if not isinstance(hourly_data.index, pd.DatetimeIndex):\n","                  raise ValueError(\"Cannot set datetime index for FCM filtering - 'datetime' column missing and index is not datetime.\")\n","             # If index is already datetime, we are good to proceed\n","\n","        # Ensure index is DatetimeIndex before filtering\n","        if not isinstance(hourly_data.index, pd.DatetimeIndex):\n","             raise TypeError(\"Index must be DatetimeIndex for filtering in FCM section.\")\n","\n","        train_indices_dt = hourly_data.index[hourly_data.index.month.isin(TARGET_MONTHS) & hourly_data.index.day.isin(TRAIN_DAYS)]\n","        # Use iloc with get_indexer for integer positions\n","        train_loc_indices = hourly_data.index.get_indexer(train_indices_dt)\n","        train_loc_indices = train_loc_indices[train_loc_indices != -1] # Remove -1 if index mismatch occurred\n","\n","        if len(train_loc_indices) > FCM_N_CLUSTERS:\n","            fcm_train_data_subset = fcm_data[:, train_loc_indices]\n","\n","            fcm_scaler = StandardScaler()\n","            fcm_train_data_scaled_subset = fcm_scaler.fit_transform(fcm_train_data_subset.T).T\n","            fcm_data_scaled = fcm_scaler.transform(fcm_data.T).T\n","\n","            print(f\"Training FCM with {FCM_N_CLUSTERS} clusters on {fcm_train_data_scaled_subset.shape[1]} samples...\")\n","            try:\n","                cntr, u_train, _, _, _, _, _ = cmeans(\n","                    fcm_train_data_scaled_subset, FCM_N_CLUSTERS, FCM_M, error=FCM_ERROR, maxiter=FCM_MAXITER, init=None, seed=42\n","                )\n","                print(\"FCM training complete.\")\n","\n","                print(\"Predicting FCM memberships for all data...\")\n","                u_all, _, _, _, _, _ = cmeans_predict(\n","                    fcm_data_scaled, cntr, FCM_M, error=FCM_ERROR, maxiter=FCM_MAXITER, seed=42\n","                )\n","\n","                fcm_membership_cols = [f'fcm_cluster_{i}' for i in range(FCM_N_CLUSTERS)]\n","                # Use original hourly_data index (which should be datetime) for the DataFrame\n","                fcm_membership_df = pd.DataFrame(u_all.T, index=hourly_data.index, columns=fcm_membership_cols)\n","                # Join back to hourly_data\n","                hourly_data = hourly_data.join(fcm_membership_df)\n","                print(f\"Added {FCM_N_CLUSTERS} FCM membership features.\")\n","                if hourly_data[fcm_membership_cols].isnull().values.any():\n","                    print(\"Warning: NaNs detected in FCM membership columns after adding. Filling with 0.\")\n","                    hourly_data[fcm_membership_cols] = hourly_data[fcm_membership_cols].fillna(0)\n","\n","            except Exception as e:\n","                print(f\"Error during FCM processing: {e}. Skipping FCM features.\")\n","                fcm_membership_cols = []\n","        else:\n","            print(f\"Warning: Not enough training data found for FCM ({len(train_loc_indices)} samples) based on filtering. Need > {FCM_N_CLUSTERS}. Skipping FCM.\")\n","            fcm_membership_cols = []\n","# --- End FCM ---\n","\n","# --- Apply Standard Fuzzification (Hour, Lag1, DayType, Season, Temp) ---\n","print(\"\\nApplying standard fuzzification (Hour, Lag1 Power, DayType, Season, Temp)...\")\n","cols_for_fuzzify_base = ['hour_original', 'dayofweek', 'month', 'temperature']\n","cols_for_fuzzify_lags = [f'{zc}_lag1' for zc in zone_columns if f'{zc}_lag1' in hourly_data.columns]\n","cols_for_fuzzify = [col for col in cols_for_fuzzify_base if col in hourly_data.columns] + cols_for_fuzzify_lags\n","\n","if not all(c in hourly_data.columns for c in cols_for_fuzzify_base):\n","     print(\"Warning: Not all base columns for fuzzification found. Skipping fuzzification for missing ones.\")\n","if not cols_for_fuzzify_lags:\n","     print(\"Warning: No Lag1 columns found for fuzzification.\")\n","\n","if cols_for_fuzzify:\n","    # Apply fuzzification using the DataFrame index to ensure alignment\n","    # Pass the DataFrame itself to apply, it will handle rows\n","    fuzzy_features_df = hourly_data[cols_for_fuzzify].apply(lambda row: fuzzify_data_enhanced(row, zone_columns), axis=1)\n","    # Explicitly set the index of the new DataFrame to match hourly_data\n","    fuzzy_features_df.index = hourly_data.index\n","    # Concatenate standard fuzzy features using join to align on index\n","    hourly_data = hourly_data.join(fuzzy_features_df, rsuffix='_fuzzy_result') # Add suffix if potential overlap\n","    # Check for NaNs introduced by fuzzification join\n","    fuzzy_cols_added = fuzzy_features_df.columns\n","    if hourly_data[fuzzy_cols_added].isnull().values.any():\n","        print(\"Warning: NaNs detected in standard fuzzy features after adding. Filling with 0.\")\n","        hourly_data[fuzzy_cols_added] = hourly_data[fuzzy_cols_added].fillna(0)\n","\n","else:\n","    print(\"Skipping standard fuzzification as required columns are missing.\")\n","# --- End Standard Fuzzification ---\n","\n","# Final check for NaNs before splitting\n","if hourly_data.isnull().values.any():\n","    print(\"Warning: NaNs detected in DataFrame before filtering. Dropping rows with any NaNs.\")\n","    initial_rows = len(hourly_data)\n","    # Identify columns with NaNs for debugging\n","    nan_cols_before_drop = hourly_data.columns[hourly_data.isnull().any()].tolist()\n","    print(f\"Columns with NaNs before drop: {nan_cols_before_drop}\")\n","    hourly_data.dropna(inplace=True)\n","    print(f\"Dropped {initial_rows - len(hourly_data)} rows due to NaNs.\")\n","\n","if len(hourly_data) == 0: raise ValueError(\"Data is empty after final NaN drop before filtering.\")\n","\n","# Ensure datetime index is still set after potential drops/joins\n","if not isinstance(hourly_data.index, pd.DatetimeIndex):\n","    # If the index was lost (e.g., by joining), try to set it again\n","    if 'datetime' in hourly_data.columns:\n","        print(\"Resetting datetime index...\")\n","        hourly_data['datetime'] = pd.to_datetime(hourly_data['datetime'])\n","        hourly_data.set_index('datetime', inplace=True)\n","    # If index is not datetime AND datetime column doesn't exist, error\n","    else:\n","         # Check if maybe reset_index happened implicitly and 'datetime' exists in columns\n","         if 'datetime' in hourly_data.columns:\n","              hourly_data.set_index('datetime', inplace=True)\n","              print(\"Set datetime index from existing column.\")\n","         else:\n","              # Check if level_0 might be the index\n","              if 'level_0' in hourly_data.columns and isinstance(hourly_data['level_0'].iloc[0], pd.Timestamp):\n","                   print(\"Using 'level_0' as datetime index.\")\n","                   hourly_data.rename(columns={'level_0':'datetime'}, inplace=True)\n","                   hourly_data.set_index('datetime', inplace=True)\n","              else:\n","                   raise ValueError(\"Datetime index lost and cannot be reliably reset before filtering.\")\n","\n","\n","print(f\"Data shape after ALL preprocessing, FCM & Enhanced Fuzzy: {hourly_data.shape}\")\n","\n","\n","# --- Filtering Data ---\n","print(f\"\\nFiltering data for final train/test subsets (TRAIN_DAYS={list(TRAIN_DAYS)}, TEST_DAYS={list(TEST_DAYS)})...\")\n","# Datetime index should already be set correctly here\n","full_train_data = hourly_data[(hourly_data.index.month.isin(TARGET_MONTHS)) & (hourly_data.index.day.isin(TRAIN_DAYS))].copy()\n","full_test_data = hourly_data[(hourly_data.index.month.isin(TARGET_MONTHS)) & (hourly_data.index.day.isin(TEST_DAYS))].copy()\n","\n","if full_train_data.empty: raise ValueError(\"Full Train data is empty after filtering. Check TARGET_MONTHS/TRAIN_DAYS and data availability.\")\n","if full_test_data.empty: raise ValueError(f\"Full Test data is empty after filtering. Check TARGET_MONTHS/TEST_DAYS (currently {list(TEST_DAYS)}) and data availability.\")\n","\n","print(f\"Full Train shape: {full_train_data.shape}, Full Test shape: {full_test_data.shape}\")\n","\n","\n","# --- Feature Preparation Function (Includes FCM + Enhanced Fuzzy) ---\n","def prepare_features_targets(train_df, test_df=None):\n","    print(\"Preparing features and targets (FCM + Enhanced Fuzzy)...\")\n","    # Exclude targets AND the original columns used for fuzzification/cyclical/clustering\n","    # 'datetime' column is created by reset_index, so it should be excluded here\n","    base_originals = ['hour_original', 'dayofweek', 'month', 'dayofyear', 'day', 'temperature', 'datetime']\n","    lag1_originals = [f'{zc}_lag1' for zc in zone_columns]\n","    fcm_originals_to_exclude = list(set(fcm_feature_cols)) # Unique FCM features used\n","\n","    # Also exclude original weather columns if they exist and are not needed directly\n","    weather_originals = [col for col in weather_cols if col != 'temperature'] # Temp is in base_originals\n","\n","    features_to_exclude = zone_columns + base_originals + lag1_originals + fcm_originals_to_exclude + weather_originals\n","    features_to_exclude = sorted(list(set(features_to_exclude))) # Unique sorted list\n","\n","    # Ensure exclusion list only contains columns actually present in the train_df\n","    features_to_exclude_present_train = [col for col in features_to_exclude if col in train_df.columns]\n","    print(f\"Columns being excluded as features: {features_to_exclude_present_train}\")\n","\n","    train_features = train_df.drop(columns=features_to_exclude_present_train, errors='ignore')\n","    train_targets = train_df[zone_columns]\n","    train_features_encoded = train_features # No explicit encoding needed here\n","    train_cols = train_features_encoded.columns.tolist()\n","    print(f\"Number of final features in training set: {len(train_cols)}\")\n","    # print(\"Final Training Features:\", train_cols) # Uncomment to debug features\n","\n","    test_features_encoded = None; test_targets = None\n","    if test_df is not None:\n","        # Ensure test_df has the same columns available before dropping\n","        features_to_exclude_present_test = [col for col in features_to_exclude if col in test_df.columns]\n","        test_features = test_df.drop(columns=features_to_exclude_present_test, errors='ignore')\n","        test_targets = test_df[zone_columns]\n","        test_features_encoded = test_features\n","        test_cols = test_features_encoded.columns.tolist()\n","        print(f\"Initial number of features in test set: {len(test_cols)}\")\n","\n","        # Align columns - Crucial step\n","        missing_in_test = set(train_cols) - set(test_cols)\n","        for c in missing_in_test:\n","            print(f\"Adding missing column '{c}' to test set (filled with 0).\")\n","            test_features_encoded[c] = 0\n","\n","        extra_in_test = set(test_cols) - set(train_cols)\n","        if extra_in_test:\n","            print(f\"Dropping extra columns from test set: {extra_in_test}\")\n","            test_features_encoded = test_features_encoded.drop(columns=list(extra_in_test))\n","\n","        # Final alignment\n","        try:\n","            test_features_encoded = test_features_encoded[train_cols] # Reorder test columns to match train\n","            print(f\"Final number of features in test set (aligned): {len(test_features_encoded.columns)}\")\n","        except KeyError as e:\n","            print(f\"ERROR: Column alignment failed. Missing columns in test set after adding: {e}\")\n","            print(\"Train columns:\", train_cols)\n","            print(\"Test columns before alignment:\", test_cols)\n","            raise\n","\n","\n","    # Check for NaN/inf values after preparation\n","    if train_features_encoded.isnull().values.any() or np.isinf(train_features_encoded.values).any():\n","        print(\"ERROR: NaN or Inf found in training features AFTER preparation. Check feature engineering steps.\")\n","        print(\"Columns with NaNs/Infs in Training Features:\\n\", train_features_encoded.isin([np.nan, np.inf, -np.inf]).sum())\n","        raise ValueError(\"NaN/Inf in training features.\")\n","    if test_features_encoded is not None and (test_features_encoded.isnull().values.any() or np.isinf(test_features_encoded.values).any()):\n","        print(\"ERROR: NaN or Inf found in test features AFTER preparation. Check feature engineering steps.\")\n","        print(\"Columns with NaNs/Infs in Test Features:\\n\", test_features_encoded.isin([np.nan, np.inf, -np.inf]).sum())\n","        raise ValueError(\"NaN/Inf in test features.\")\n","\n","\n","    return train_features_encoded, train_targets, test_features_encoded, test_targets\n","\n","# --- Prepare data ---\n","# *** Use reset_index() here as intended, the previous fix should prevent the error ***\n","final_train_features_enc, final_train_targets, final_test_features_enc, final_test_targets = prepare_features_targets(\n","    full_train_data.reset_index(), full_test_data.reset_index()\n",")\n","\n","# --- Scaling ---\n","print(\"Scaling data (MinMaxScaler for features, StandardScaler for targets)...\")\n","# Fit scalers ONLY on the final training set\n","final_scaler_feat = MinMaxScaler(); final_scaler_targ = StandardScaler()\n","final_train_features_scaled = final_scaler_feat.fit_transform(final_train_features_enc)\n","final_test_features_scaled = final_scaler_feat.transform(final_test_features_enc)\n","final_train_targets_scaled = final_scaler_targ.fit_transform(final_train_targets)\n","# Scale test targets using the same scaler fitted on train targets for inverse transform later\n","final_test_targets_scaled = final_scaler_targ.transform(final_test_targets)\n","\n","\n","# --- Reshaping ---\n","def reshape_data(features, targets=None):\n","    # Reshape features to (samples, timesteps, features) - Here timesteps=1\n","    features_reshaped = features.reshape((features.shape[0], 1, features.shape[1]))\n","    # Targets remain 2D (samples, num_zones)\n","    return features_reshaped, targets\n","\n","final_train_feat_res, final_train_targ_res = reshape_data(final_train_features_scaled, final_train_targets_scaled)\n","final_test_feat_res, final_test_targ_res = reshape_data(final_test_features_scaled, final_test_targets_scaled) # Reshape test targets too\n","\n","print(f\"Final Train Features Shape: {final_train_feat_res.shape}, Targets Shape: {final_train_targ_res.shape}\")\n","print(f\"Final Test Features Shape: {final_test_feat_res.shape}, Targets Shape: {final_test_targ_res.shape}\")\n","\n","\n","# --- LSTM Model Building Function (Modified for 3 Layers) ---\n","def build_lstm_model(lstm_units, dense_units, dropout_rate, lr, n_features, n_outputs):\n","    \"\"\" Builds a 3-layer Bidirectional LSTM model. \"\"\"\n","    model = Sequential([\n","        Bidirectional(LSTM(lstm_units, return_sequences=True, input_shape=(1, n_features))),\n","        Dropout(dropout_rate), BatchNormalization(),\n","        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n","        Dropout(dropout_rate), BatchNormalization(),\n","        Bidirectional(LSTM(lstm_units, return_sequences=False)),\n","        Dropout(dropout_rate), BatchNormalization(),\n","        Dense(dense_units, activation='relu'),\n","        Dense(n_outputs)\n","    ])\n","    optimizer = Adam(learning_rate=lr)\n","    model.compile(loss='mean_squared_error', optimizer=optimizer)\n","    return model\n","\n","# ==============================================================================\n","# --- Final Model Training (Using Fixed Parameters - 3 LSTM Layers) ---\n","# ==============================================================================\n","print(f\"\\nBuilding and training FINAL model (FCM + Enhanced Fuzzy, 3 LSTM Layers) using FIXED parameters...\")\n","final_n_features = final_train_feat_res.shape[2]; final_n_outputs = final_train_targ_res.shape[1]\n","K.clear_session()\n","\n","# Use the fixed hyperparameters defined earlier\n","final_model = build_lstm_model(\n","    FIXED_LSTM_UNITS,\n","    FIXED_DENSE_UNITS,\n","    FIXED_DROPOUT,\n","    FIXED_LEARNING_RATE,\n","    final_n_features,\n","    final_n_outputs\n",")\n","print(\"Using Fixed Hyperparameters:\")\n","print(f\"  LSTM Units (x3 Layers): {FIXED_LSTM_UNITS}\")\n","print(f\"  Dense Units: {FIXED_DENSE_UNITS}\")\n","print(f\"  Dropout Rate: {FIXED_DROPOUT:.3f}\")\n","print(f\"  Learning Rate: {FIXED_LEARNING_RATE:.6f}\")\n","\n","final_model.summary()\n","\n","# Define final training callbacks\n","final_early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n","final_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=8, min_lr=1e-7, verbose=1)\n","\n","# Train the final model\n","history_final = final_model.fit(\n","    final_train_feat_res,\n","    final_train_targ_res,\n","    epochs=150, # Using 150 epochs as per previous request\n","    batch_size=32,\n","    validation_split=0.2,\n","    callbacks=[final_early_stopping, final_reduce_lr],\n","    verbose=1\n",")\n","\n","# --- Plot Final Training Loss ---\n","plt.figure(figsize=(10, 6))\n","plt.plot(history_final.history['loss'], label='Training Loss')\n","plt.plot(history_final.history['val_loss'], label='Validation Loss')\n","plt.title(f'FINAL Model Loss (FCM + Enhanced Fuzzy, 3x{FIXED_LSTM_UNITS} LSTM, Fixed Params)')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss (MSE)')\n","plt.legend()\n","plt.grid(True)\n","# Adjust ylim for better visualization\n","if len(history_final.history['loss']) > 5:\n","    max_loss_val = max(history_final.history['loss'][5:] + history_final.history['val_loss'][5:])\n","    min_loss_val = min(history_final.history['loss'] + history_final.history['val_loss'])\n","    plt.ylim(bottom=max(0, min_loss_val * 0.9), top=max_loss_val * 1.1)\n","elif len(history_final.history['loss']) > 0:\n","     max_loss_val = max(history_final.history['loss'] + history_final.history['val_loss'])\n","     min_loss_val = min(history_final.history['loss'] + history_final.history['val_loss'])\n","     plt.ylim(bottom=max(0, min_loss_val * 0.9), top=max_loss_val * 1.1)\n","else:\n","    plt.ylim(bottom=0)\n","plt.show()\n","\n","if hasattr(final_early_stopping, 'stopped_epoch') and final_early_stopping.stopped_epoch > 0 and final_early_stopping.restore_best_weights:\n","    print(f\"Final training stopped early at epoch {final_early_stopping.stopped_epoch + 1}. Best weights restored.\")\n","elif hasattr(final_early_stopping, 'stopped_epoch') and final_early_stopping.stopped_epoch > 0:\n","     print(f\"Final training stopped early at epoch {final_early_stopping.stopped_epoch + 1}, but best weights might not have been restored.\")\n","else:\n","    print(\"Final training completed all epochs or early stopping did not trigger.\")\n","\n","\n","# --- Final Evaluation ---\n","print(f\"\\nEvaluating FINAL model (FCM + Enhanced Fuzzy, 3x{FIXED_LSTM_UNITS} LSTM, Fixed Params) on Test Set (Days {list(TEST_DAYS)})...\")\n","test_predictions_scaled = final_model.predict(final_test_feat_res)\n","\n","# Inverse transform predictions and actuals\n","actual_test_targets = final_scaler_targ.inverse_transform(final_test_targ_res)\n","predicted_test_targets = final_scaler_targ.inverse_transform(test_predictions_scaled)\n","# Clip negative predictions\n","predicted_test_targets[predicted_test_targets < 0] = 0\n","\n","# --- Create plotting dataframe ---\n","plot_ready = False\n","# Check length using the index of full_test_data and the length of predictions\n","if len(full_test_data.index) == len(actual_test_targets):\n","    # Create DataFrames for actual and predicted values using the test data index\n","    actual_df = pd.DataFrame(actual_test_targets, index=full_test_data.index, columns=zone_columns)\n","    pred_df = pd.DataFrame(predicted_test_targets, index=full_test_data.index, columns=[f\"{zone}_predicted\" for zone in zone_columns])\n","\n","    # Combine index, actuals, and predictions into a new DataFrame for plotting\n","    plot_test_df = pd.concat([actual_df, pred_df], axis=1)\n","    plot_test_df['hour'] = plot_test_df.index.hour\n","    plot_test_df['day'] = plot_test_df.index.day # Use .day\n","    plot_test_df['month_name'] = plot_test_df.index.strftime('%B')\n","    plot_test_df['day_name'] = plot_test_df.index.day_name()\n","\n","    plot_ready = True\n","    print(f\"Plotting dataframe created successfully.\")\n","else:\n","    print(f\"Error: Length mismatch! Original Test DF index length {len(full_test_data.index)} vs Predictions length {len(actual_test_targets)}. Skipping plotting.\")\n","    plot_test_df = None\n","\n","# --- Plotting Results ---\n","if plot_ready and plot_test_df is not None:\n","    print(f\"Plotting daily forecasts for FINAL model (FCM + Enhanced Fuzzy, 3x{FIXED_LSTM_UNITS} LSTM, Fixed Params) for days {list(TEST_DAYS)}...\")\n","    unique_month_names = plot_test_df['month_name'].unique()\n","\n","    for zone in zone_columns:\n","        zone_name_plot = zone.replace(\"_power_consumption\", \"\").replace(\"zone_\", \"Zone \").capitalize()\n","\n","        for month_name in unique_month_names:\n","            month_data = plot_test_df[plot_test_df['month_name'] == month_name]\n","            if month_data.empty: continue\n","\n","            available_days_in_plot_df = month_data['day'].unique()\n","            plot_days = sorted([d for d in TEST_DAYS if d in available_days_in_plot_df])\n","\n","            for day in plot_days:\n","                filtered_data = month_data[month_data['day'] == day]\n","                if filtered_data.empty: continue\n","\n","                actual_col_name = zone\n","                predicted_col_name = f\"{zone}_predicted\"\n","                if actual_col_name not in filtered_data.columns or predicted_col_name not in filtered_data.columns:\n","                    print(f\"Warning: Column missing for {zone}, {month_name}, {day}. Skipping plot.\")\n","                    continue\n","\n","                actual_day = filtered_data[actual_col_name]\n","                predicted_day = filtered_data[predicted_col_name]\n","\n","                if not isinstance(actual_day, pd.Series) or not isinstance(predicted_day, pd.Series) or actual_day.ndim > 1 or predicted_day.ndim > 1:\n","                     print(f\"Warning: Invalid data shape/type for metrics/plot for {zone}, {month_name}, {day}. Skipping.\")\n","                     continue\n","\n","                try:\n","                    daily_rmse = sqrt(mean_squared_error(actual_day, predicted_day))\n","                    daily_mae = mean_absolute_error(actual_day, predicted_day)\n","                    daily_r2 = r2_score(actual_day, predicted_day)\n","                except ValueError as e_met:\n","                    print(f\"Warning: Metric calculation error for {zone}, {month_name}, {day}: {e_met}\")\n","                    daily_rmse, daily_mae, daily_r2 = np.nan, np.nan, np.nan\n","                except Exception as e_gen:\n","                    print(f\"Unexpected error calculating metrics for {zone}, {month_name}, {day}: {e_gen}\")\n","                    daily_rmse, daily_mae, daily_r2 = np.nan, np.nan, np.nan\n","\n","\n","                plt.figure(figsize=(14, 7))\n","                bar_width = 0.35\n","                x = np.arange(len(filtered_data))\n","                plt.bar(x - bar_width/2, actual_day, width=bar_width, label='Actual', color='black', alpha=0.8)\n","                plt.bar(x + bar_width/2, predicted_day, width=bar_width, label='Predicted', color='orange', alpha=0.8)\n","\n","                day_name_str = filtered_data['day_name'].iloc[0] if not filtered_data.empty else \"UnknownDay\"\n","                plt.title(f\"{zone_name_plot} Load Forecast (FCM+Fuzzy, 3x{FIXED_LSTM_UNITS} LSTM) - {day_name_str}, {month_name} {day}\", fontsize=14)\n","                plt.xlabel('Hour of Day', fontsize=12)\n","                plt.ylabel('Power Consumption', fontsize=12)\n","                plt.xticks(x, filtered_data['hour'], rotation=45, ha='right')\n","                plt.legend(fontsize=10)\n","                plt.grid(axis='y', linestyle='--', alpha=0.7)\n","                plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout\n","\n","                stats_text = (f\"Daily Metrics:\\n\"\n","                              f\"RMSE: {daily_rmse:.2f}\\n\"\n","                              f\"MAE:  {daily_mae:.2f}\\n\"\n","                              f\"R²:   {daily_r2:.3f}\")\n","                plt.text(1.02, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,\n","                         verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', facecolor='orange', alpha=0.6))\n","                plt.show()\n","else:\n","    print(\"Skipping plots due to data length mismatch or plotting dataframe creation failed.\")\n","\n","\n","# --- Final Evaluation Metrics ---\n","print(f\"\\n--- FINAL Model (FCM + Enhanced Fuzzy, 3x{FIXED_LSTM_UNITS} LSTM, Fixed Params): Test Set Evaluation Metrics (Per Zone) ---\")\n","all_actuals_flat = []\n","all_predictions_flat = []\n","\n","for i, zone in enumerate(zone_columns):\n","    zone_name_print = zone.replace(\"zone_\", \"Zone_\").replace(\"_power_consumption\", \"_\")\n","    actual = actual_test_targets[:, i]\n","    predicted = predicted_test_targets[:, i]\n","    all_actuals_flat.extend(actual)\n","    all_predictions_flat.extend(predicted)\n","\n","    rmse = sqrt(mean_squared_error(actual, predicted))\n","    mae = mean_absolute_error(actual, predicted)\n","    try:\n","        r2 = r2_score(actual, predicted)\n","    except ValueError:\n","        r2 = np.nan\n","\n","    print(f\"\\n--- {zone_name_print} ---\")\n","    print(f\"  RMSE: {rmse:.2f}\")\n","    print(f\"  MAE: {mae:.2f}\")\n","    print(f\"  R² Score: {r2:.4f}\")\n","\n","\n","print(f\"\\n--- FINAL Model (FCM + Enhanced Fuzzy, 3x{FIXED_LSTM_UNITS} LSTM, Fixed Params): Overall Evaluation (All Zones Combined) ---\")\n","all_actuals_flat = np.array(all_actuals_flat)\n","all_predictions_flat = np.array(all_predictions_flat)\n","\n","overall_rmse = sqrt(mean_squared_error(all_actuals_flat, all_predictions_flat))\n","overall_mae = mean_absolute_error(all_actuals_flat, all_predictions_flat)\n","try:\n","    overall_r2 = r2_score(all_actuals_flat, all_predictions_flat)\n","except ValueError:\n","    overall_r2 = np.nan\n","\n","# Calculate Tolerance Accuracy Safely\n","abs_error = np.abs(all_actuals_flat - all_predictions_flat)\n","non_zero_actuals = all_actuals_flat[all_actuals_flat > 1e-6]\n","mean_nonzero_actual = np.mean(non_zero_actuals) if len(non_zero_actuals) > 0 else 1.0\n","\n","absolute_min_tolerance = 10\n","percentage_tolerance_value = (TOLERANCE_PERCENTAGE / 100.0) * np.abs(all_actuals_flat)\n","tolerance_threshold = np.maximum(percentage_tolerance_value, absolute_min_tolerance)\n","\n","is_within_tolerance = abs_error <= tolerance_threshold\n","tolerance_accuracy = np.mean(is_within_tolerance) * 100\n","\n","print(f\"  Overall RMSE: {overall_rmse:.2f}\")\n","print(f\"  Overall MAE: {overall_mae:.2f}\")\n","print(f\"  Overall R² Score: {overall_r2:.4f}\")\n","print(f\"\\n  Tolerance-Based Accuracy ({TOLERANCE_PERCENTAGE}%): {tolerance_accuracy:.2f}%\")\n","print(f\"     (Accuracy defined as % of predictions within +/- {TOLERANCE_PERCENTAGE}% of actual, with min absolute tolerance of {absolute_min_tolerance} units)\")\n","print(\"----------------------------------------------------------\")\n","\n","print(\"\\nProcessing complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1HXGYhT4GsD-6v5gNrqZRXaVEzyd6NFyL"},"id":"HJhnRY3eb0Iq","outputId":"7425cd61-65ce-4247-c2e9-fe85590b87ce","executionInfo":{"status":"ok","timestamp":1745431824077,"user_tz":300,"elapsed":48131,"user":{"displayName":"Abhisek Das","userId":"09120859727457019726"}}},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V5E1","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMUwmt0BhZOdUlmmEYfIj0f"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}